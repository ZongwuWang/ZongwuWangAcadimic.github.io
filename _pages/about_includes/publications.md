<!-- ---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
--- -->

# üìù Selected Publications 

You can also find my articles on <u><a href="https://scholar.google.com/citations?user=14_fX2wAAAAJ">my Google Scholar profile</a></u>

<!-- <font color=538F79>{=} denotes equal contribution; &emsp; {*} denotes corresponding author</font> -->

<p style="color:#538F79;">
  {=} denotes equal contribution; &emsp; {*} denotes corresponding author
</p>

<!-- After joining SJTU. -->
<table width="100%" style="table-layout:auto;" style="font-size:inherit;" >

<!-- <tr ><td bgcolor="#F7F7F7"><strong>ASPLOS 2026</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=,*, Ning Yang=, Jingkui Yang, <u><b>Zongwu Wang</b></u>, Chenyang Guan, Yu Feng, Li Jiang, and Haibing Guan<br> <a href="#">EARTH: An Efficient MoE Accelerator with Entropy-Aware Speculative Prefetch and Result Reuse</a> <strong>(Acceptance Rate: 10.6%)</strong> 
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>TACO 2025</strong><br>(Top Jour. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Ning Yang, Fangxin Liu*, Junjie Wang, Chenyang Guan, <u><b>Zongwu Wang</b></u>, Junping Zhao, Li Jiang, and Haibing Guan<br> <a> Rethinking Variable-Length Encoding: Exploiting Bit Sparsity for Parallel Decoding in LLM Accelerators</a> <strong>(CCF Tier A)</strong> 
</td></tr> -->

<tr ><td style="font-size:inherit;"><strong>TACO 2025</strong><br>(Top Jour. in Computer Architecture)</td><td style="font-size:inherit;"> Shiyuan Huang, Fangxin Liu=,*, <u><b>Zongwu Wang</b></u>, Ning Yang, Haomin Li, Haibing Guan, and Li Jiang<br> <a href="#"> MIX-PC: Enabling Efficient DNN Inference with Mixed Numeric Precision Compilation Optimization</a>  <strong>(CCF Tier A)</strong> 
</td></tr>

<!-- <tr ><td bgcolor="#F7F7F7"><strong>DATE 2026</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7">  Haomin Li, Fangxin Liu*, Chenyang Guan, <u><b>Zongwu Wang</b></u>, Li Jiang and Haibing Guan<br> <a href="#"> LaMoS: Enabling Efficient Lrge Number Modular Multiplication through SRAM-based CiM Acceleration</a> <strong>(Acceptance Rate: 25%)</strong> 
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;"><strong>AAAI 2026</strong><br>(Top Conf. in AI)</td><td style="font-size:inherit;"> Zhixiong Zhao, Fangxin Liu=*, Junjie Wang, Chenyang Guan, <u><b>Zongwu Wang</b></u>, Li Jiang, Haibing Guan <br> <a href="#"> SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization</a>  <strong>(Acceptance Rate: 17.6%)</strong> 
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>HPCA 2026</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Haomin Li, Yue Liang, Fangxin Liu=,*, Bowen Zhu, <u><b>Zongwu Wang</b></u>, Yu Feng, Liqiang Lu, Li Jiang, and Haibing Guan<br> <a href="#"> ORANGE: Exploring Ockham's Razor for Neural Rendering by Accelerating 3DGS on NPUs with GEMM-Friendly Blending and Balanced Workloads</a> <strong>(Acceptance Rate: 19.7%)</strong> 
</td></tr> -->

<tr ><td bgcolor="#F7F7F7"><strong>ASP-DAC 2026</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> <u><b>Zongwu Wang</b></u>, Zhongyi Tang, Fangxin Liu*, Chenyang Guan, Li Jiang*, Haibing Guan<br> <a href="#"> TFLOP: Towards Energy-Efficient LLM Inference via An FPGA-Affinity Accelerator with Unified LUT-based Optimization</a> <strong>(Acceptance Rate: 29%)</strong> 
</td></tr>

<!-- <tr ><td bgcolor="#F7F7F7"><strong>ASP-DAC 2026</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Yilong Zhao, Fangxin Liu*, <u><b>Zongwu Wang</b></u>, Mingjian Li, Mingxing Zhang, Chixiao Chen, Li Jiang<br> <a href="#"> BLADE: Boosting LLM Decoding's Communication Efficiency in DRAM-based PIM</a> <strong>(Acceptance Rate: 29%)</strong> 
</td></tr> -->

<tr ><td style="font-size:inherit;"><strong>EMNLP 2025</strong><br>(Top Conf. in NLP)</td><td style="font-size:inherit;"> Fangxin Liu=, <u><b>Zongwu Wang=</b></u>, Jinhong Xia, Junping Zhao*, Shouren Zhao, Jinjin Li, Jian Liu, Li Jiang*, Haibing Guan <br> <a href="#">  FlexQuant: A Flexible and Efficient Dynamic Precision Switching Framework for LLM Quantization</a>  <strong>(Acceptance Rate: 22%)</strong>  <font color=800e13><b>[Applied at Ant Group]</b></font>
</td></tr>

<!-- <tr ><td style="font-size:inherit;"><strong>ACM MM 2025</strong><br>(Top Conf. in AI)</td><td style="font-size:inherit;"> Fangxin Liu=*, Junjie Wang=, Ning Yang, <u><b>Zongwu Wang</b></u>, Junping Zhao, Li Jiang, and Haibing Guan<br> <a href="#"> ASTER: Adaptive Dynamic Layer-Skipping for Efficient Transformer Inference via Markov Decision Process</a>  <strong>(Oral)</strong> 
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>ICCAD 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Zhixiong Zhao=, Haomin Li=, Fangxin Liu*, Yuncheng Lu, <u><b>Zongwu Wang</b></u>, Tao Yang, Haibing Guan, and Li Jiang<br> <a href="#"> QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations</a> <strong>(Acceptance Rate: 24%)</strong> 
</td></tr> -->

<tr ><td style="font-size:inherit;"><strong>ICCAD 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"> Yiwei Hu, Fangxin Liu=*, <u><b>Zongwu Wang</b></u>, Yilong Zhao, Tao Yang, Haibing Guan, and Li Jiang <br> <a href="#"> PLAIN: Leveraging High Internal Bandwidth in PIM for Accelerating Large Language Model Inference via Mixed-Precision Quantization</a>  <strong>(Acceptance Rate: 24%)</strong> 
</td></tr>

<!-- <tr ><td bgcolor="#F7F7F7"><strong>ASPLOS 2026</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7">Fangxin Liu=*, Haomin Li=, Bowen Zhu, <u><b>Zongwu Wang</b></u>, Zhuoran Song, Haibing Guan, and Li Jiang <br> <a href="#"> ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering</a> <strong>(Acceptance Rate: 23%)</strong> 
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>TACO 2025</strong><br>(Top Jour. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Haomin Li, Fangxin Liu*, <u><b>Zongwu Wang</b></u>, Ning Yang, Shiyuan Huang, Xiaoyao Liang, and Li Jiang <br> <a href="https://dl.acm.org/doi/pdf/10.1145/3736172"> Attack and Defense: Enhancing Robustness of Binary Hyper-Dimensional Computing</a> <strong>(CCF Tier A)</strong> 
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;"><strong>ISCA 2025</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;">  Haomin Li=, Fangxin Liu=*,  Yichi Chen, <u><b>Zongwu Wang</b></u>, Shiyuan Huang, Ning Yang, Dongxu Lyu, and Li Jiang<br> <a href="https://dl.acm.org/doi/full/10.1145/3695053.3731031"> FATE: Boosting the Performance of Hyper-Dimensional Computing Intelligence with Flexible Numerical DAta TypE</a>  <strong>(Acceptance Rate: 21%)</strong> 
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;"><strong>DAC 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;">  Fangxin Liu=,  Haomin Li=, <u><b>Zongwu Wang</b></u>, Bo Zhang, Mingzhe Zhang, Shoumeng Yan, Li Jiang, and Haibing Guan<br> <a href="http://arxiv.org/abs/2503.15916"> ALLMod: Exploring Area-Efficiency of LUT-based Large Number Modular Reduction via Hybrid Workloads</a>  <strong>(Acceptance Rate: 23%)</strong> 
</td></tr> -->

<tr ><td bgcolor="#F7F7F7"><strong>DAC 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> <u><b>Zongwu Wang=</b></u>, Peng Xu=, Fangxin Liu*, Yiwei Hu, Qingxiao Sun, Gezi Li, Cheng Li, Xuan Wang, Li Jiang, and Haibing Guan <br> <a href="https://arxiv.org/abs/2504.03661"> MILLION: MasterIng Long-Context LLM InferenceVia Outlier-Immunized KV Product OuaNtization</a> <strong>(Acceptance Rate: 23%)</strong>  <font color=800e13><b>[Applied at HUAWEI]</b></font>
</td></tr>

<!-- <tr ><td style="font-size:inherit;"><strong>DAC 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"> Fangxin Liu=*, Ning Yang=, <u><b>Zongwu Wang</b></u>, Xuanpeng Zhu, Haidong Yao, Xiankui Xiong, Li Jiang and Haibing Guan <br> <a href="#"> BLOOM: Bit-Slice Framework for DNN Acceleration with Mixed-Precision</a>  <strong>(Acceptance Rate: 23%)</strong>  <font color=800e13><b>[Applied at ZTE]</b></font>
</td></tr> -->

<tr ><td bgcolor="#F7F7F7"><strong>DAC 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Ning Yang, <u><b>Zongwu Wang*</b></u>, Qingxiao Sun, Liqiang Lu, and Fangxin Liu <br> <a href="#"> PISA:Efficient Precision-Slice Framework forLLMs with Adaptive Numerical Type</a>  <strong>(Acceptance Rate: 23%)</strong>  
</td></tr>

<!-- <tr ><td style="font-size:inherit;"><strong>DATE 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;">  Fangxin Liu=, Haomin Li=, <u><b>Zongwu Wang</b></u>, Dongxu Lyu, and Li Jiang<br> <a href="#"> HyperDyn: Dynamic Dimensional Masking forEffcient Hyper-Dimensional Computing</a>  <strong>(Acceptance Rate: 21%)</strong> 
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>DATE 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Haomin Li=, Zewen Sun, <u><b>Zongwu Wang</b></u>, and Li Jiang <br> <a href="#"> HyperNeO: Efficient and AccurateHyper-Dimensional Regression via Neural Optimization</a> <strong>(Acceptance Rate: 21%)</strong>  
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;"><strong>DATE 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"> Haomin Li=, Fangxin Liu=, <u><b>Zongwu Wang</b></u>, Dongxu Lyu, Shiyuan Huang, Ning Yang, Qi Sun, Zhuoran Song, and Li Jiang<br> <a href="#"> TAIL: Exploiting Temporal Asynchronous Execution for Efficient Spiking Neural Networks with Inter-Layer Parallelism</a>  <strong>(Acceptance Rate: 21%)</strong> 
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>DATE 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Ning Yang=, <u><b>Zongwu Wang</b></u>, Xuanpeng Zhu, Haidong Yao, Xiankui Xiong, Qi Sun, and Li Jiang <br> <a href="#"> OPS: Outlier-aware Precision-Slice Framework for LLM Acceleration</a>  <strong>(Acceptance Rate: 21%)</strong>  
</td></tr> -->

<tr ><td style="font-size:inherit;"><strong>DATE 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"> <u><b>Zongwu Wang</b></u>, Fangxin Liu, Peng Xu, Qingxiao Sun, Junping Zhao and Li Jiang <br> <a href="#">EVASION: Efficient KV Cache Compression via Product Quantization</a>  <strong>(Acceptance Rate: 21%)</strong> 
</td></tr>

<!-- <tr ><td bgcolor="#F7F7F7"><strong>HPCA 2025</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Shiyuan Huang=, Ning Yang, <u><b>Zongwu Wang</b></u>, Haomin Li, and Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10946829"> CROSS: Compiler-Driven Optimization of Sparse DNNs Using  Sparse/Dense Computation Kernels</a> <strong>(Acceptance Rate: 21%)</strong> 
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>IEEE TCAS-AI 2024</strong><br>(Imp. Jour. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Ning Yang=, Fangxin Liu=, <u><b>Zongwu Wang</b></u>, Junping Zhao, Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10744537"> SearchQ: Search-based Fine-Grained Quantization for Data-Free Model Compression</a>  <font color=800e13><b>[Applied at HUAWEI]</b></font>
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;"><strong>IEEE TODAES 2024</strong><br>(Imp. Jour. in Design Automation)</td><td style="font-size:inherit;"> Shiyuan Huang=, Fangxin Liu=, Tian Li, <u><b>Zongwu Wang</b></u>, Ning Yang, Haomin Li and Li Jiang <br> <a href="https://dl.acm.org/doi/abs/10.1145/3701033"> STCO: Enhancing Training Efficiency via Structured Sparse Tensor Compilation Optimization</a>  <strong>(CCF Tier B)</strong>  <font color=800e13><b>[Applied at Yizhu Tech.]</b></font>
</td></tr> -->

<tr ><td bgcolor="#F7F7F7"><strong>ASP-DAC 2025</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, <u><b>Zongwu Wang=</b></u>, Peng Xu, Shiyuan Huang and Li Jiang <br> <a href="https://dl.acm.org/doi/abs/10.1145/3658617.3697565"> Exploiting Differential-Based Data Encoding for Enhanced Query Efficiency</a>  <strong>(Acceptance Rate: 28%)</strong>
</td></tr>

<!-- <tr ><td style="font-size:inherit;"><strong>ASP-DAC 2025</strong><br>(Top. Conf. in Design Automation)</td><td style="font-size:inherit;"> Haomin Li=, Fangxin Liu=, Zewen Sun, <u><b>Zongwu Wang</b></u>, Shiyuan Huang, Ning Yang, and Li Jiang <br> <a href="https://dl.acm.org/doi/abs/10.1145/3658617.3697716"> NeuronQuant: Accurate and Efficient Post-Training Quantization for Spiking Neural Networks</a>  <strong>(Acceptance Rate: 28%)</strong>
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>IEEE TCAD 2024</strong><br>(Top Journal in Computer-Aided Design)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Shiyuan Huang, Fangxin Liu*, Tao Yang, <u><b>Zongwu Wang</b></u>, Ning Yang, and Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10640142"> SpMMPlu-Pro: An Enhanced Compiler Plug-In for Efficient SpMM and Sparsity Propagation Algorithm</a>  <strong>(CCF Tier A)</strong>
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;"><strong>ICCD 2024</strong><br>(Import. Conf. in Computer Architecture)</td><td style="font-size:inherit;"> Fangxin Liu=, Ning Yang=, <u><b>Zongwu Wang</b></u>, Zhiyan Song, Tao Yang, and Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10818081"> T-BUS: Taming Bipartite Unstructured Sparsity for Energy-Effcient DNN Acceleration</a>  <strong>(Acceptance Rate: 25%)</strong>
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>ICCD 2024</strong><br>(Import. Conf. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Ning Yang=,Zhiyan Song, <u><b>Zongwu Wang</b></u> and Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10817951"> HOLES: Boosting Large Language Models Efficiency with Hardware-friendly Lossless Encoding</a>  <strong>(Acceptance Rate: 25%)</strong>
</td></tr> -->


<tr ><td style="font-size:inherit;"><strong>ICCD 2024</strong><br>(Import. Conf. in Computer Architecture)</td><td style="font-size:inherit;"> <u><b>Zongwu Wang=</b></u>, Fangxin Liu=, and Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10818108"> PS4:A Low Power SNN Accelerator with Spike Speculative Scheme</a>  <strong>(Acceptance Rate: 25%)</strong>
</td></tr>


<tr ><td bgcolor="#F7F7F7"><strong>ICCD 2024</strong><br>(Import. Conf. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Longyu Zhao, <u><b>Zongwu Wang</b></u>, Fangxin Liu*, and Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10818016"> Ninja: A Hardware Assisted System for Accelerating Nested Address Translation</a>  <strong>(Acceptance Rate: 25%)</strong>
</td></tr>


<tr ><td style="font-size:inherit;"><strong>MICRO 2024</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;"> <u><b>Zongwu Wang</b></u>, Fangxin Liu*, Ning Yang, Shiyuan Huang, Haomin Li, and Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10764497"> COMPASS: SRAM-Based Computing-in-Memory SNN Accelerator with  Adaptive Spike Speculation</a>  <strong>(Acceptance Rate: 22%)</strong>
</td></tr>

<tr ><td style="font-size:inherit;"><strong>IEEE TPDS 2024</strong><br>(Top Journal in Computer Architecture)</td><td style="font-size:inherit;"> Fangxin Liu, <u><b>Zongwu Wang</b></u>, Wenbo Zhao, Ning Yang, Yongbiao Chen, Shiyuan Huang, Haomin Li, Tao Yang, Songwen Pei, Xiaoyao Liang,and Li Jiang <br> <a href="https://ieeexplore.ieee.org/document/10561563"> Exploiting Temporal-Unrolled Parallelism for Energy-Efficient SNN Acceleration</a>  <strong>(CCF Tier A)</strong>
</td></tr>


<tr ><td bgcolor="#F7F7F7"><strong>ISLPED 2024</strong><br>(Top Conf. in Low Power Design)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> <u><b>Zongwu Wang</b></u>, Fangxin Liu*, Longyu Zhao, Shiyuan Huang and Li Jiang <br> <a href="https://dl.acm.org/doi/pdf/10.1145/3665314.3672279"> LowPASS: A Low power PIM-based accelerator with Speculative Scheme for SNNs</a>  <strong>(Acceptance Rate: 21%)</strong>
</td></tr>

<tr ><td style="font-size:inherit;"><strong>ISCA 2024</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;"> Yilong Zhao, Mingyu Gao, Fangxin Liu*, Yiwei Hu, <u><b>Zongwu Wang</b></u>, Han Lin, Ji Li, He Xian, Hanlin Dong, Tao Yang, Naifeng Jing, Xiaoyao Liang, Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10609641"> UM-PIM: DRAM-based PIM with Uniform & Shared Memory Space</a>  <strong>(Acceptance Rate: 18%)</strong>
</td></tr>


<!-- <tr ><td bgcolor="#F7F7F7"><strong>DAC 2024</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Ning Yang=, Haomin Li, <u><b>Zongwu Wang</b></u>, Zhuoran Song, Songwen Pei, Li Jiang <br> <a href="https://dl.acm.org/doi/abs/10.1145/3649329.3655896"> INSPIRE: Accelerating Deep Neural Networks via Hardware-friendly Index-Pair Encoding</a>  <strong>(Acceptance Rate: 23%)</strong>
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;"><strong>DAC 2024</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"> Fangxin Liu=, Ning Yang=, Haomin Li, <u><b>Zongwu Wang</b></u>, Zhuoran Song, Songwen Pei, Li Jiang <br> <a href="https://dl.acm.org/doi/abs/10.1145/3649329.3655981">EOS: An Energy-Oriented Attack Framework for Spiking Neural Networks</a>  <strong>(Acceptance Rate: 23%)</strong>
</td></tr> -->

<!-- <tr ><td bgcolor="#F7F7F7"><strong>HPCA 2024</strong><br>(Top Conf. in Computer Architecture)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Ning Yang=, Haomin Li, <u><b>Zongwu Wang</b></u>, Zhuoran Song, Songwen Pei, Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10476472">SPARK: Scalable and Precision-Aware Acceleration of Neural  Networks via Efficient Encoding</a>  <strong>(Acceptance Rate: 18%)</strong>
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;" ><strong>ASPDAC 2024</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;" > Fangxin Liu=, Haomin Li=, Ning Yang, Yichi Chen, <u><b>Zongwu Wang</b></u>, Tao Yang, Li Jiang <br> <a href="https://ieeexplore.ieee.org/abstract/document/10473823">PAAP-HD: PIM-Assisted Approximation for Efficient Hyper-Dimensional Computing</a> <strong>(Acceptance Rate: 29%)</strong>
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>ASPDAC 2024</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Haomin Li=, Ning Yang, <u><b>Zongwu Wang</b></u>, Tao Yang, Li Jiang <br> <a href="https://ieeexplore.ieee.org/document/10473984">TEAS: Exploiting Spiking Activity for Temporal-wise Adaptive Spiking Neural Networks</a>  <strong>(Acceptance Rate: 29%)</strong>
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;" ><strong>ASPDAC 2024</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;" > Shiyuan Huang=, Fangxin Liu=, Tian Li, <u><b>Zongwu Wang</b></u>, Haomin Li, Li Jiang<br> <a href="https://ieeexplore.ieee.org/abstract/document/10473981">TSTC: Enabling Efficient Training via Structured Sparse Tensor Compilation</a>  <strong>(Acceptance Rate: 29%)</strong>
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;" ><strong>IEEE TC 2023</strong><br>(Top Journal in Computer Architecture)</td><td style="font-size:inherit;" > Fangxin Liu, Wenbo Zhao, <u><b>Zongwu Wang</b></u>, Yongbiao Chen, Xiaoyao Liang, Li Jiang<br> <a href="https://ieeexplore.ieee.org/document/10177200">ERA-BS: Boosting the Efficiency of ReRAM-based PIM Accelerator with Fine-Grained Bit-Level Sparsity</a>  <strong>(CCF Tier A)</strong>
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>DAC 2024</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Haomin Li=, <u><b>Zongwu Wang</b></u>, Yongbiao Chen, Li Jiang<br> <a href="https://ieeexplore.ieee.org/abstract/document/10247811">HyperAttack: An Efficient Attack Framework for HyperDimensional Computing</a>  <strong>(Acceptance Rate: 23%)</strong>
</td></tr> -->

<tr ><td style="font-size:inherit;" ><strong>ICCD 2022</strong></td><td style="font-size:inherit;" > Fangxin Liu, <u><b>Zongwu Wang</b></u>, Yongbiao Chen, Li Jiang<br> <a href="https://ieeexplore.ieee.org/abstract/document/9978437">Randomize and Match: Exploiting Irregular Sparsity for Energy Efficient Processing in SNNs</a>  <strong>(Acceptance Rate: 24%)</strong>
</td></tr>


<tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>IEEE TCAD 2022</strong><br>(Top Journal in Computer-Aided Design)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu, <u><b>Zongwu Wang</b></u>, Yongbiao Chen, Zhezhi He, Tao Yang, Xiaoyao Liang, Li Jiang<br> <a href="https://ieeexplore.ieee.org/document/9769275">SoBS-X: Squeeze-Out Bit Sparsity for ReRAM-Crossbar-Based Neural Network Accelerator</a>  <strong>(CCF Tier A)</strong>
</td></tr>


<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>IEEE TCAD 2022</strong><br>(Top Journal in Computer-Aided Design)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Wenbo Zhao, <u><b>Zongwu Wang</b></u>, Yilong Zhao, Tao Yang, Yiran Chen, Li Jiang<br> <a href="https://ieeexplore.ieee.org/document/972425">IVQ: In-Memory Acceleration of DNN Inference Exploiting Varied Quantization</a>  <strong>(CCF Tier A)</strong>
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;" ><strong>DAC 2022</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;" > Fangxin Liu, Wenbo Zhao, <u><b>Zongwu Wang</b></u>, Yongbiao Chen, Zhezhi He, Naifeng Jing, Xiaoyao Liang, Li Jiang<br> <a href="/files/%5BDAC-2022%5DEBSP_preprint.pdf">EBSP: Evolving Bit Sparsity Patterns for Hardware Friendly Inference of Quantized Deep Neural Networks</a>  <strong>(Acceptance Rate: 24.7%)</strong> 
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>DAC 2022</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Wenbo Zhao, Yongbiao Chen, <u><b>Zongwu Wang</b></u>, Zhezhi He, Rui Yang, Qidong Tang, Tao Yang, Cheng Zhuo<br> <a href="/files/%5BDAC-2022%5DPIM-DH_preprint.pdf">PIM-DH: ReRAM based Processing in Memory Architecture for Deep Hashing Acceleration</a> <strong>(Acceptance Rate: 24.7%)</strong> 
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;" ><strong>DAC 2022</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;" > Fangxin Liu, Wenbo Zhao, <u><b>Zongwu Wang</b></u>, Yongbiao Chen, Tao Yang, Zhezhi He, Xiaokang Yang, Li Jiang<br> <a href="/files/%5BDAC-2022%5DSATO_preprint.pdf">SATO: Spiking Neural Network Acceleration via Temporal Oriented Dataflow and Architecture</a> <strong>(Acceptance Rate: 24.7%)</strong> 
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>ICASSP 2022</strong><br>(Top Conf. in Signal Processing)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Wenbo Zhao, Yongbiao Chen, <u><b>Zongwu Wang</b></u>, Fei Dai<br> <a href="/files/%5BICASSP-22%5DDynSNN_preprint.pdf">Dynsnn: A dynamic approach to reduce redundancy in spiking neural networks</a> <strong>(CCF Tier B)</strong> 
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;" ><strong>AAAI'22 (Oral)</strong><br>(Top Conf. in Artificial Intelligence)</td><td style="font-size:inherit;" > Fangxin Liu, Wenbo Zhao*, Yongbiao Chen, <u><b>Zongwu Wang</b></u>, Li Jiang<br> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20061">SpikeConverter: An Efficient Conversion Framework Zipping the Gap between Artificial Neural Networks and Spiking Neural Networks</a> <strong>(Acceptance Rate: 15%)</strong> 
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>Frontiers in Neuroscience, 2021</strong><br>(SCI Tier 2)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Wenbo Zhao=, Yongbiao Chen, <u><b>Zongwu Wang</b></u>, Tao Yang, Li Jiang<br> <a href="https://www.frontiersin.org/articles/10.3389/fnins.2021.756876/full">SSTDP: Supervised Spike Timing Dependent Plasticity for Efficient Spiking Neural Network Training</a> <strong>(Impact Factor: 4.7)</strong> 
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;" ><strong>ICCD 2021</strong></td><td style="font-size:inherit;" > Fangxin Liu, Wenbo Zhao, Zhezhi He, <u><b>Zongwu Wang</b></u>, Yilong Zhao, Tao Yang, Jingnai Feng, Xiaoyao Liang, Li Jiang<br> <a href="https://ieeexplore.ieee.org/document/9643646">SME: ReRAM-based Sparse-Multiplication-Engine to Squeeze-Out Bit Sparsity of Neural Network</a> <strong>(Acceptance Rate: 24.4%)</strong> 
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>ICCV 2021</strong><br>(Top Conf. in Computer Vision)</td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Wenbo Zhao, Zhezhi He, Yanzhi Wang, <u><b>Zongwu Wang</b></u>, Changzhi Dai, Xiaoyao Liang, Li Jiang<br> <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Improving_Neural_Network_Efficiency_via_Post-Training_Quantization_With_Adaptive_Floating-Point_ICCV_2021_paper.html">Improving Neural Network Efficiency via Post-training Quantization with Adaptive Floating-Point</a> <strong>(Acceptance Rate: 25.9%)</strong> 
</td></tr> -->

<!-- <tr ><td style="font-size:inherit;" ><strong>ICCAD 2021</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;" > Fangxin Liu, Wenbo Zhao, Zhezhi He, <u><b>Zongwu Wang</b></u>, Yilong Zhao, Yongbiao Chen, Li Jiang<br> <a href="https://ieeexplore.ieee.org/document/9643569">Bit-Transformer: Transforming Bit-level Sparsity into Higher Preformance in ReRAM-based Accelerator</a> <strong>(Acceptance Rate: 23.5%)</strong> 
</td></tr> -->


<!-- <tr ><td style="font-size:inherit;"  bgcolor="#F7F7F7"><strong>GLSVLSI 2021</strong></td><td style="font-size:inherit;"  bgcolor="#F7F7F7"> Fangxin Liu=, Wenbo Zhao, <u><b>Zongwu Wang</b></u>, Tao Yang, Li Jiang<br> <a href="https://dl.acm.org/doi/abs/10.1145/3453688.3461491">IM3A: Boosting Deep Neural Network Efficiency via In-Memory Addressing-Assisted Acceleration</a> <strong>(Acceptance Rate: 24%)</strong> 
</td></tr> -->


<tr ><td style="font-size:inherit;" ><strong>DATE 2022</strong><br>(Top Conf. in Design Automation)</td><td style="font-size:inherit;" > <u><b>Zongwu Wang</b></u>, Zhezhi He, Rui Yang, Shiquan Fan, Jie Lin, Fangxin Liu, Yueyang Jia, Chenxi Yuan, Qidong Tang, Li Jiang.<br> <a href="#">Self-Terminated Write of Multi-Level Cell ReRAM for Efficient Neuromorphic Computing</a> <strong>(Best Paper Award)</strong>
</td></tr>


</table>
